{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.sans-serif']=[u'SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets.california_housing import fetch_california_housing\n",
    "from sklearn import tree\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#观察大局\n",
    "#模型想要根据其他指标，预测任意区域的房价中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#框架选择\n",
    "#1.询问业务目标是什么，公司如何使用这个模型， 如何从中获益。\n",
    "#这决定了我们怎么设定问题，选择什么算法，使用什么测量方式来评估模型的性能，以及该花多少力气来进行调整。下游系统会根据此决策来决定是否值的投资，\n",
    "#这会直接影响收益。\n",
    "#2.当前解决方案，如果有的话是什么。\n",
    "#可以用来当作参考，也能获得解决问题的洞察。\n",
    "#目前由一个专家团队手动估算区域的住房价格，一个团队持续收集最新的区域信息，然后使用复杂的规则来进行估算，即昂贵又耗时，还由高达15%的误差率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设计系统\n",
    "#1.我们首先回答框架问题，是监督/无监督/强化学习，分类/回归？批量学习/增量学习？\n",
    "#显然，是监督学习，因为给出了标记的训练集，回归任务，因为需要预测，是多变量回归，需要用到多个特征进行预测，最后，我们目前没有一个连续的数据流\n",
    "#流进系统，数据量也不是很大，不需要多个内存，因此使用批量学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择性能指标\n",
    "#rmse or mahantten distance? 他们均为包含n个元素的向量vk的范数。一个是k=2,一个是k=1 。k值越大， 越关注大的值，忽略小的值。因此rmse对异常值更加敏感。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查假设\n",
    "#列举和验证到目前为止的假设，是个非常良好的习惯，这可以在初期检查出严重问题。例如：我们假设是结果会被下游系统使用，但如果下游系统需要将价格\n",
    "#转换为类别（比如廉价，中等，或昂贵），转而使用类别而不是价格本身。这时候并不需要完全准确的预估价格，只需要给出类别就可以了。这种情况下\n",
    "#我们就应该做一个分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT='http://raw.githubusercontent.com/ageron/handson-ml/master'\n",
    "HOUSING_PATH=os.path.join('datasets','housing')\n",
    "HOUSING_URL=DOWNLOAD_ROOT+'datasets/housing/housing.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.fetch_housing_data(housing_url='http://raw.githubusercontent.com/ageron/handson-ml/masterhousing.tgz', housing_path='datasets/housing')>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def fetch_housing_data(housing_url=HOUSING_URL,housing_path=HOUSING_PATH):\n",
    "    #os.makedirs(housing_path,exist_ok=True)\n",
    "    #tgs_path=os.path.join(housing_path,'housing.tgs')\n",
    "    #urllib.request.urlretrieve(housing_url,tgs_path)\n",
    "   # housing_tgs=tarfile.open(tgs_path)\n",
    "   # housing_tgs.extractall(path=housing_path)\n",
    "   # housing_tgs.close()\n",
    "#fetch_housing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看数据结构\n",
    "housing = fetch_california_housing()\n",
    "housing.data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#树模型参数：\n",
    "#1.criterion gini or entropy\n",
    "\n",
    "#2.splitter best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中（数据量大的时候）\n",
    "\n",
    "#3.max_features None（所有），log2，sqrt，N 特征小于50的时候一般使用所有的\n",
    "\n",
    "#4.max_depth 数据少或者特征少的时候可以不管这个值，如果模型样本量多，特征也多的情况下，可以尝试限制下\n",
    "\n",
    "#5.min_samples_split 如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。\n",
    "\n",
    "#6.min_samples_leaf 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值，大些如10W可是尝试下5\n",
    "\n",
    "#7.min_weight_fraction_leaf 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。\n",
    "\n",
    "#8.max_leaf_nodes 通过限制最大叶子节点数，可以防止过拟合，默认是\"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到。\n",
    "\n",
    "#9.class_weight 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。\n",
    "\n",
    "#10.min_impurity_split 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 。\n",
    "\n",
    "#n_estimators:要建立树的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr=tree.DecisionTreeRegressor(max_depth=2)\n",
    "#使用两列的特征进行训练\n",
    "dtr.fit(housing.data[:, [6, 7]], housing.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(dtr, out_file = None, feature_names = housing.feature_names[6:8],filled = True, impurity = False,rounded = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.get_nodes()[7].set_fillcolor(\"#FFF2DD\")\n",
    "#graph.write_png(\"graph.png\")\n",
    "#Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8097021394052101"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#划分训练集测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size = 0.1, random_state = 42)\n",
    "dtr = tree.DecisionTreeRegressor(random_state=42)\n",
    "dtr.fit(x_train, y_train)\n",
    "dtr.score(x_test, y_test)\n",
    "#使用随机森林\n",
    "rfr = RandomForestRegressor( random_state = 42)\n",
    "rfr.fit(x_train, y_train)\n",
    "rfr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'grid_scores_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9d4954e25ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'grid_scores_'"
     ]
    }
   ],
   "source": [
    "#用交叉验证选参数\n",
    "#把参数写成字典的形式\n",
    "tree_param_grid = { 'min_samples_split': list((3, 6, 9)),'n_estimators': list((10,50,100))}\n",
    "# 第一个参数是模型，第二个参数是待选的参数，cv:进行几次交叉验证\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid = tree_param_grid, cv = 5)\n",
    "grid.fit(x_train, y_train)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将得到的参数重新训练随机森林\n",
    "rfr = RandomForestRegressor( min_samples_split=3,n_estimators = 100,random_state = 42)\n",
    "rfr.fit(x_train, y_train)\n",
    "rfr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把结果按重要性输出\n",
    "pd.Series(rfr.feature_importances_, index = housing.feature_names).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.ones((2,3))\n",
    "b=np.power(x,range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "param=np.ones((3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 3., 3.],\n",
       "       [3., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b@param"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
